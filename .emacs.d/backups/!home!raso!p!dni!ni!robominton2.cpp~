#include<iostream>
#include <fstream>
#include <sstream>
#include<stdexcept>
#include<OpenNI.h>
#include<opencv2/opencv.hpp>
#include <opencv2/core/core.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/contrib/contrib.hpp> //時間計測のためのライブラリ

#include <stdio.h>
#include <math.h>
#include <string>
#include <stdlib.h>



#define PI  3.14159265258979

#define sq(x) ((x)*(x))

#define len(array) (sizeof(array) / sizeof(array[0]))

#define FRAME_WIDTH 320 //画像サイズ 横
#define FRAME_HEIGHT 240 //    ;     縦

#define MAX_DEPTH 4000 // 取得する距離の最大値
#define MIN_DEPTH 500  //       ;       最小値


#define MAX_SIZE 120   // 白領域の最大値 
#define MIN_SIZE 5     //      ;  最小値


#define WHITE_MIN 0x00 // 白色と判定する最小値
#define WHITE_DIF 50   // RGBそれぞれの差異の最小値

#define ABS_MAX 20 

#define COUNT_MAX 15

#define GF  (-9.8 /1000)
#define M_K  (5.0 / 0.24 * 1000)

#define Phi(a) PI / 360 * a

#define PHI (Phi(-35))

#define GY  (GF * cosf(PHI))
#define GZ  (GF * sinf(PHI))

#define CAMERA_H  2000



using namespace cv;
using namespace std;

class DepthSensor
{
private:

	void changeResolution(openni::VideoStream& stream)
		{
			openni::VideoMode mode = stream.getVideoMode();
			mode.setResolution(FRAME_WIDTH, FRAME_HEIGHT);
			mode.setFps(60);
			stream.setVideoMode(mode);
			stream.setMirroringEnabled(false);
		}

public:
	void initialize(){
		
		//デバイスを習得する
		openni::Status ret = device.open( openni::ANY_DEVICE );
		if ( ret != openni::STATUS_OK){
			throw std::runtime_error( "openni::Device::open() failed.");
		}
			
		//シャトルの座標 初期化
		ShuttlePoint[0] = 0; 
		ShuttlePoint[1] = 0;
		ShuttlePoint[2] = 0;

		
		Count = 0;
		isExist = 0;

		//カラーストリームを有効にする
		colorStream.create(device, openni::SENSOR_COLOR);
		//changeResolution(colorStream);
		//colorStream.setMirroringEnabled(true);
		colorStream.start();

		//Depthストリームを有効にする
		depthStream.create(device, openni::SENSOR_DEPTH);
			
		depthStream.start();
				
	}

	//フレームの更新処理
	void update()
		{
			isExist = 1;

			static int backup_point;
			backup_point = ShuttlePoint[0];
			

			openni::VideoFrameRef colorFrame;
			openni::VideoFrameRef depthFrame;
			

			//更新されたフレームを習得する
			colorStream.readFrame( &colorFrame );
			depthStream.readFrame( &depthFrame );
			

			//フレームのデータを表示できる形に変換する
			colorImage = showColorStream( colorFrame );
//			depthImage = showDepthStream( depthFrame );


			monoImage = colorImage.clone();
			monoImage = thresh(monoImage, depthFrame);
			textImage = Mat::zeros(240, 320, CV_8UC3);

			stringstream ss;

			
			ss << "( " << ShuttlePoint[0] <<", " << ShuttlePoint[1] << ", " << ShuttlePoint[2] << ")"; 
			putText(textImage, ss.str(), Point(30, 30),
					FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255));

			locateImage = Mat::zeros(240, 320, CV_8UC3);
			
			drawLine(locateImage);

			cvNamedWindow("Color Stream", CV_WINDOW_NORMAL);
//			cvResizeWindow("Color Stream", FRAME_WIDTH*2 , FRAME_HEIGHT*2);
			cvMoveWindow("Color Stream",0, 0);

			cvNamedWindow("Locate Stream" ,CV_WINDOW_NORMAL);
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);
			cvMoveWindow("Locate Stream",500, 0);

			
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);


//			openni::VideoFrameRef testFrame;
//			testStream.readFrame( &testFrame );

			cvNamedWindow("Mono Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Mono Stream",0, 400);

			cvNamedWindow("Text Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Text Stream",500, 400);


			cvtColor(monoImage, monoImage,  CV_BGR2GRAY); //グレースケール変換(ここでは白黒画像のチャンネルをかえているだけ）
//			threshold(testImage, testImage, 0, 255, THRESH_BINARY|THRESH_OTSU);
			
			testImage = monoImage.clone();

			testImage = get_shuttle(testImage, colorImage, depthFrame);

			imshow("Color Stream", colorImage);
			imshow("Locate Stream", locateImage);
			imshow("Mono Stream", monoImage);
			imshow("Text Stream", textImage);

//			showCenterDistance(depthImage, depthFrame);

			if(Count > COUNT_MAX) Count = 0;

			//シャトルの位置が移動しているかどうか
			if(backup_point == ShuttlePoint[0])
				{
					isExist = 0;
					Count++;
				}
			else{
				Count=0;
			}
		}


	
	void drawLine(Mat image){
		line(image, Point(160, 0), cv::Point(160, 240), Scalar(255,255,255), 1, 4);  
		circle(image,Point(160, 300), 100, Scalar(255,255,255), 1, 4);
		circle(image,Point(160, 300), 200, Scalar(255,255,255), 1, 4);
		circle(image,Point(320 - ShuttlePoint[0],240 - 240*(ShuttlePoint[2] - MIN_DEPTH)/(MAX_DEPTH - MIN_DEPTH)), 10, Scalar(255,105,105), -1, 4);
	
	}

	//画像上のピクセルがWHITE_MINより明るいかどうか
	int is_white( Mat color_image ,int x,int y){
		
		if((color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] > WHITE_MIN) && 
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] > WHITE_MIN) &&
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] > WHITE_MIN)) 
			return 1;

		else return 0;
	}

	// RGBのどれかが強くなっていないかどうか（白に近いかどうか）
	int is_white2(Mat color_image, int x, int y){
		int c_flag = 0;
		if(is_white( color_image, x, y)){
			for(int i = 0; i < 3; i++){
				
				if(abs(
					   color_image.data[ y * color_image.step + x * color_image.elemSize() + i%3]  
					   - color_image.data[ y * color_image.step + x * color_image.elemSize() + (i+1)%3] 
					   ) < WHITE_DIF)
					
					c_flag++;
			
			}
		}
		
		return (c_flag == 3)? 1 : 0;
	}

	//MAX_DEPTHとMIN_DEPTH間の距離の領域を白、それ以外を黒にする
	Mat thresh(Mat color_image, const openni::VideoFrameRef& depthframe){

		unsigned short* depth = (unsigned short*)depthframe.getData();

		int whitecount = 0;
		
		for(int x = 0; x < FRAME_WIDTH; x++){
			for(int y = 0; y < FRAME_HEIGHT; y++){
				int de = depth[(y * FRAME_WIDTH) + x];
				if((de < MAX_DEPTH && de > MIN_DEPTH) && is_white2(color_image, x, y)){
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0xff;
					
					whitecount++;

			}

				else{

					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0;
		
				}
			}
		}
		if(whitecount > FRAME_WIDTH * FRAME_HEIGHT / 3)
			return Mat::zeros(240, 320, CV_8UC3);
		
		else return color_image;




	}
	
	//二値化画像から白領域（シャトル）の座標を取得
	Mat get_shuttle(Mat bin_img ,Mat color_image, const openni::VideoFrameRef& depthframe){
		
		int cx, cy;
		unsigned short cz;
		size_t max = 0;
		int max_i = 0;

		vector<vector<Point> > contours;

		findContours(bin_img ,contours, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE); //輪郭抽出

		for(int i = 0; i < (int)contours.size(); i++){
			size_t count = contours[i].size();
			if(count < MIN_SIZE || count > MAX_SIZE) continue; //白領域が小さすぎる/大きすぎるものは除外
			if(max < count) {
				max = count;
				max_i = i;
			}

			Rect rc = boundingRect(contours[max_i]); //白領域を囲む短形を生成

			//短形が正方形から離れすぎていないかどうか（白領域の形が上下に長すぎないか / シャトルの形かどうか）
			if( abs((rc.br().x - rc.tl().x) - (rc.br().y - rc.tl().y)) < ABS_MAX){


				cx = (rc.tl().x + rc.br().x) /2; //短形の…
				cy = (rc.tl().y + rc.br().y) /2; //…中心点
				cz = PointDistance(cx, cy , depthframe); //短形の中心点の深度座標



				ShuttlePoint[0] = cx;
				ShuttlePoint[1] = cy;
				ShuttlePoint[2] = cz;

				rectangle(color_image, rc.tl(), rc.br(), Scalar(0,0,200), 2, 4); //短形を画像上に表示
						

						break;
//					}
			}
		}
		
		return bin_img;
		
	}


	//カラーストリームを表示できる形にする
	Mat showColorStream(const openni::VideoFrameRef& colorframe)
		{
			//OpenCVの形に変換する
			Mat colorImage = Mat(colorframe.getHeight(),
								 colorframe.getWidth(),
								 CV_8UC3, (unsigned char*)colorframe.getData());

			//BGRの並びをRGBに変換する
			cvtColor(colorImage, colorImage, CV_RGB2BGR);

			return colorImage;
		}

	Mat showDepthStream(const openni::VideoFrameRef& depthframe){
		//距離データを画像化する(16bit)
		Mat depthImage = Mat(depthframe.getHeight(),
							 depthframe.getWidth(),
							 CV_16UC1, (unsigned short*)depthframe.getData());

		//0 〜 10000ミリメートルまでのデータを0〜255(8bit)にする
		depthImage.convertTo(depthImage, CV_8U, 255.0 / 10000);

		//中心点の距離を表示する
		showCenterDistance(depthImage, depthframe);

		return depthImage;
	}

	void showCenterDistance(Mat& depthImage, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();


		int centerX = videoMode.getResolutionX() / 2;
		int centerY = videoMode.getResolutionY() / 2;
		int centerIndex = (centerY * videoMode.getResolutionX()) + centerX;

		unsigned short* depth = (unsigned short*)depthframe.getData();


		stringstream ss;
		stringstream ss1;

		ss << "Center Point: " << depth[centerIndex];
	}
	
	//(x,y)の画素の深度を取得
	unsigned short PointDistance(int x, int y, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();

		int pointIndex = (y * videoMode.getResolutionX()) + x;

		unsigned short* depth = (unsigned short*)depthframe.getData();

		return depth[pointIndex];


	}

	int ShuttlePoint[3]; //シャトルの座標 ShuttlePoint([0],[1],[2]) = (x,y,z)
	int Count; // 画像上にシャトルが存在しないフレームをカウントする変数
	int isExist; // 画像上にシャトルが存在するフラグ変数


private:

	openni::Device device;
	openni::VideoStream colorStream;
	openni::VideoStream depthStream;

	
	
	Mat colorImage;
	Mat depthImage;
	Mat testImage;
	Mat monoImage;
	Mat textImage;
	Mat locateImage;

};


string IntToString(int number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}

string DoubleToString(double number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}

void Pers(int befor_log[2][4],float conduct_log[2][4]){
	float F = 542.37;  //xtionの画像を320ｘ240 mm^2としたときのxtionと画像の距離
	
	for(int i = 0; i< 2; i++){
		conduct_log[i][0] = float((befor_log[i][0]*2 - 320)*befor_log[i][2]/F);
		conduct_log[i][1] = float((befor_log[i][1]*2 - 240)*befor_log[i][2]/F);
		conduct_log[i][2] = float(befor_log[i][2]);
		conduct_log[i][3] = float(befor_log[i][3]);
	}
}

float DropTime(float V0, float H,float Z){

	float t,Hmax,Z_Hmax,D,D0;

	float max2G,max2H;

	t = -1*M_K * logf(M_K*GY/(V0 + M_K*GY));

		//print "t = " + str(t)

	Hmax = H + ( M_K * ((V0 + M_K * GY)*(1 - expf(-1 * 1 / M_K * t)) - GY * t));
	Z_Hmax = Z + M_K * ((V0 + M_K * GZ)*(1 - expf(-1 * 1 / M_K * t)) - GZ * t);

	printf ("TEST8.1 M_K=%f GY=GY V0=%f\n", M_K,GY,V0);
	printf ("TEST8 %f\n", M_K*GY / (V0 + (M_K*GY)));
	printf ("TEST7 %f\n",t);
	printf ("TEST6 %f\n",expf(-1 * 1 / (M_K * t)));
	printf ("TEST4 %f\n",Hmax);
	printf ("TEST5 %f\n",Z_Hmax);
	
	/*
	print "H = " + str(H)
	print "Hmax = " + str(Hmax)
	print "Z_Hmax = " + str(Z_Hmax)
	*/


	D = abs(-tanf(PHI) * Z_Hmax + Hmax - CAMERA_H) / sqrtf(sq(tanf(PHI))  + 1);
	D0 = abs(-tanf(PHI) * Z + H - CAMERA_H) / sqrtf(sq(tanf(PHI))  + 1);
	
	
	max2G = sqrtf(-1 * M_K/GY) * acoshf(expf(D/M_K));
	max2H = sqrtf(-1 * M_K/GY) * acoshf(expf((D - D0)/cosf(PHI)/M_K));


	return max2G - max2H;

}

void MarkPoint(float Tlog[2][4],float T,float mark[3]){

	float V0[3] = {0.0,0.0,0.0};

	float X,Y,Z;

	for(int i = 0; i < 3; i++)
		V0[i] = float(Tlog[1][i] - Tlog[0][i]) / Tlog[1][3];

//(0,2,1)になってるの注意
	mark[0] = M_K * V0[0] * (1 - expf(-1 / M_K * T)) + Tlog[0][0];

	mark[2] = M_K * ((V0[2] + M_K * GZ)*(1 - expf(-1 * 1 / M_K * T)) - GZ * T) + Tlog[0][2];
	
	mark[1] = M_K * ((V0[1] + M_K * GY)*(1 - expf(-1 * 1 / M_K * T)) - GY * T) + Tlog[0][1];


}



int main(int arg, const char * argv[])
{

	FILE *fp;

	char row[256];

	float V0,T;

	//int now_point[2][3] = {{0,0,0},{0,0,0}};
	int now_point[2][4] = {{200,100,1000,100},{300,200,1500,200}};

	float conduct_point[2][4] = {{0,0,0,0},{0,0,0,0}};

	float mark[3];
	float GroundPoint[2] = {0,0};

	int serial_connect = 0;


	if ((fp = fopen("/dev/ttyUSB0", "r+")) == NULL)
		printf("***NOT CONNECT SERIAL PORT!\n");

	else{
		printf("***OK\n");
		serial_connect = 1;
	}

	int push_space = 0; //Spaceキー フラグ
	string str;
	int push_key = 0;
	int serial_say = 0;


    //シリアル通信用データ変数
	int go_x  = 100, go_y = 100; 
	int push_x = 100, push_y = 100;
	int centerx = FRAME_WIDTH/2, centery = FRAME_HEIGHT/2;

	TickMeter meter;

	if(remove("log.txt") != 0){
		printf("***log.txtがありません\n");
		return 0;
	}



	ofstream out("log.txt");
	if (!out) {
		cout << "***error  出力ファイルを開けません\n";
		//exit(1);
	}

	Pers(now_point, conduct_point);

	printf("TEST %f,%f,%f,%f\n",conduct_point[0][0],conduct_point[0][1],conduct_point[0][2],conduct_point[0][3]);
	printf("TEST %f,%f,%f,%f\n",conduct_point[1][0],conduct_point[1][1],conduct_point[1][2],conduct_point[1][3]);

	V0 = (conduct_point[1][1] - conduct_point[0][1])/conduct_point[1][3];
	
	printf("TEST3 %f\n", V0);
	T = DropTime(V0, conduct_point[0][1] ,conduct_point[0][2]);
	printf("TEST2 %f\n", T);

	MarkPoint(conduct_point,T,mark);
	printf("TEST Mark =(%f,%f,%f)\n",mark[0],mark[1],mark[2]);
	GroundPoint[0] = mark[0];
	GroundPoint[1] = sqrt(sq(mark[1] - CAMERA_H)  + sq(mark[2]));
	printf("TEST GroundPoint =(%f,%f)\n",GroundPoint[0],GroundPoint[1]);
	try{
		//OpenNIを初期化する
		openni::OpenNI::initialize();

		//センサーを初期化する
		DepthSensor sensor;
		sensor.initialize();
		sensor.update();

		now_point[1][0] = sensor.ShuttlePoint[0];
		now_point[1][1] = sensor.ShuttlePoint[1];
		now_point[1][2] = sensor.ShuttlePoint[2];
		
		if(serial_connect)
			fprintf(fp,"A100100\n");

		
		printf ("TEST9 %f\n",PHI);
		printf ("TEST10 %f\n",GY);

		//メインループ
		while(1){
			sensor.update();

			
			now_point[0][0] = now_point[1][0]; 
			now_point[0][1] = now_point[1][1]; 
			now_point[0][2] = now_point[1][2];

			now_point[1][0] = sensor.ShuttlePoint[0];
			now_point[1][1] = sensor.ShuttlePoint[1];
			now_point[1][2] = sensor.ShuttlePoint[2];

			/*
			Pers(now_point, conduct_point);

			V0 = (conduct_point[1][1] -conduct_point[0][1])/conduct_point[1][3];

			T = DropTime(V0, conduct_point[0][1] ,conduct_point[0][2]);
			*/
		
			
			if(sensor.Count == COUNT_MAX && push_space){
				if(serial_connect)
					fprintf(fp,"A100100\n");
				printf("A100100\n");
			}

			
			if(serial_connect && fgets(row,sizeof(row),fp) != NULL)
			{
				//printf ("%s\n", row);
				serial_say = int(row[0]);
				printf("%d\n" ,serial_say);
				if(serial_say != 1 || serial_say != 0)
				{
					serial_say = 0;
					printf("***SERIAL_RANGE_FALSE\n");
					
				}	

				else 
					printf("***************");
			}

			if(sensor.isExist == 1) {

				if(push_space){

					meter.stop();

					
					printf("(%d, %d, %d)\n",
						   sensor.ShuttlePoint[0],
						   sensor.ShuttlePoint[1],
						   sensor.ShuttlePoint[2] );

					cout << meter << endl;


					out << "(" << IntToString(sensor.ShuttlePoint[0]) 
						<< ", " << IntToString(sensor.ShuttlePoint[1]) 
						<< ", " << IntToString(sensor.ShuttlePoint[2]) 
						<< ", " << DoubleToString(meter.getTimeMilli()) 
						<< ")" << endl;

					// シリアル通信用データ生成
					
					if(sensor.ShuttlePoint[2] > 1500) go_y = 200;
					else go_y = (sensor.ShuttlePoint[2] - 500) /10+100;

					meter.reset();

					go_x = 200 - go_x;

					if(serial_connect)
						fprintf(fp,"A%6d\n",go_x * 1000 + go_y);
					printf("A%6d\n",go_x *1000 + go_y);

					meter.start();
									
				}
		
				if(sensor.Count == COUNT_MAX)
					printf("-----\n");
			}

			
			
			//Enterでキーボード操縦モード
			if(waitKey(10) == 13)
				while(1){
					
					push_key = waitKey(100);

					if(push_key == 97)
						push_x = 0;
						
					
					else if(push_key == 100)
						push_x = 200;
						
					else
						push_x = 100;
															
					if(push_key == 115)
						push_y=0;
					
					else if(push_key == 119)
						push_y =200;
						
					
					else
						push_y =100;
						

					if(push_key == 9) break;
					fprintf(fp,"A%6d\n",push_x * 1000 + push_y);
					printf("***KEY MODE\n");

				}
			if(waitKey(10) == 32 || serial_say == 1)
				
				push_space = 1;

			else
				push_space = 0;
			
			if(waitKey(10) == 27){
				
				fprintf(fp,"A100100\n");
				
				break;
			}
			
			//printf("%d\n",waitKey(10));
		}}
	catch(std::exception&){
		std::cout << openni::OpenNI::getExtendedError() << std::endl;
	}

	out.close();

	fclose(fp);

	return 0;
}
	
			   
