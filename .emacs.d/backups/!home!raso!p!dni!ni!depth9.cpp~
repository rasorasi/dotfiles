#include<iostream>
#include <fstream>
#include <sstream>
#include<stdexcept>
#include<OpenNI.h>
#include<opencv2/opencv.hpp>
#include <opencv2/core/core.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/contrib/contrib.hpp> //時間計測のためのライブラリ

#include <stdio.h>
#include <math.h>
#include <string>
#include <stdlib.h>


#define FRAME_WIDTH 320
#define FRAME_HEIGHT 240

#define MAX_DEPTH 3000
#define MIN_DEPTH 500


#define MIN_SIZE 5
#define MAX_SIZE 120


#define WHITE_MIN 0x00
#define WHITE_DIF 50

#define ABS_MAX 20

#define COUNT_MAX 15



using namespace cv;
using namespace std;

class DepthSensor
{
private:

	void changeResolution(openni::VideoStream& stream)
		{
			openni::VideoMode mode = stream.getVideoMode();
			mode.setResolution(FRAME_WIDTH, FRAME_HEIGHT);
			mode.setFps(60);
			stream.setVideoMode(mode);
			stream.setMirroringEnabled(false);
		}

public:
	void initialize(){
		
		//デバイスを習得する
		openni::Status ret = device.open( openni::ANY_DEVICE );
		if ( ret != openni::STATUS_OK){
			throw std::runtime_error( "openni::Device::open() failed.");
		}
			
		ShuttlePoint[0] = 0;
		ShuttlePoint[1] = 0;
		ShuttlePoint[2] = 0;

		Count = 0;
		isExist = 0;

		//カラーストリームを有効にする
		colorStream.create(device, openni::SENSOR_COLOR);
		//changeResolution(colorStream);
		//colorStream.setMirroringEnabled(true);
		colorStream.start();

		//Depthストリームを有効にする
		depthStream.create(device, openni::SENSOR_DEPTH);
			
		depthStream.start();
				
	}

	//フレームの更新処理
	void update()
		{
			isExist = 1;

			static int backup_point;
			backup_point = ShuttlePoint[0];
			

			openni::VideoFrameRef colorFrame;
			openni::VideoFrameRef depthFrame;
			

			//更新されたフレームを習得する
			colorStream.readFrame( &colorFrame );
			depthStream.readFrame( &depthFrame );
			

			//フレームのデータを表示できる形に変換する
			colorImage = showColorStream( colorFrame );
//			depthImage = showDepthStream( depthFrame );


			monoImage = colorImage.clone();
			monoImage = thresh(monoImage, depthFrame);
			textImage = Mat::zeros(240, 320, CV_8UC3);

			stringstream ss;

			ss << "( " << ShuttlePoint[0] <<", " << ShuttlePoint[1] << ", " << ShuttlePoint[2] << ")"; 
			putText(textImage, ss.str(), Point(30, 30),
					FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255));

			locateImage = Mat::zeros(240, 320, CV_8UC3);

			drawLine(locateImage);

			cvNamedWindow("Color Stream", CV_WINDOW_NORMAL);
//			cvResizeWindow("Color Stream", FRAME_WIDTH*2 , FRAME_HEIGHT*2);
			cvMoveWindow("Color Stream",0, 0);

			cvNamedWindow("Locate Stream" ,CV_WINDOW_NORMAL);
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);
			cvMoveWindow("Locate Stream",500, 0);

			
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);


//			openni::VideoFrameRef testFrame;
//			testStream.readFrame( &testFrame );

			cvNamedWindow("Mono Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Mono Stream",0, 400);

			cvNamedWindow("Text Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Text Stream",500, 400);

			

			cvtColor(monoImage, monoImage,  CV_BGR2GRAY); //グレースケール変換(ここでは白黒画像のチャンネルをかえているだけ）
//			threshold(testImage, testImage, 0, 255, THRESH_BINARY|THRESH_OTSU);
			
			testImage = monoImage.clone();

			testImage = get_shuttle(testImage, colorImage, depthFrame);

			imshow("Color Stream", colorImage);
			imshow("Locate Stream", locateImage);
			imshow("Mono Stream", monoImage);
			imshow("Text Stream", textImage);

//			showCenterDistance(depthImage, depthFrame);

			if(Count > COUNT_MAX) Count = 0;

			if(backup_point == ShuttlePoint[0])
				{
					isExist = 0;
					Count++;
				}
			else{
				Count=0;
			}
		}

	void drawLine(Mat image){
		line(image, Point(160, 0), cv::Point(160, 240), Scalar(255,255,255), 1, 4);  
		circle(image,Point(160, 300), 100, Scalar(255,255,255), 1, 4);
		circle(image,Point(160, 300), 200, Scalar(255,255,255), 1, 4);
		circle(image,Point(320 - ShuttlePoint[0],240 - 240*(ShuttlePoint[2] - MIN_DEPTH)/(MAX_DEPTH - MIN_DEPTH)), 10, Scalar(255,105,105), -1, 4);
	
	}


	Mat showTestStream(const openni::VideoFrameRef& depthframe,const openni::VideoFrameRef& colorframe){
		//距離データを画像化する(16bit)
		unsigned short* depth = (unsigned short*)depthframe.getData();		
	
		Mat colorimage = Mat(colorframe.getHeight(),
							 colorframe.getWidth(),
							 CV_8UC3, (unsigned short*)colorframe.getData());

		for(int x = 0; x < FRAME_WIDTH; x++){
			for(int y = 0; y < FRAME_HEIGHT; y++){
				int de = depth[(y * FRAME_WIDTH) + x];
				if(de > MAX_DEPTH || de < MIN_DEPTH){
					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 0] = 0;
					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 1] = 0;
					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 2] = 0;

			}

				else{

					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 0] = 0xff;
					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 1] = 0xff;
					colorimage.data[ y * colorimage.step + x * colorimage.elemSize() + 2] = 0xff;
		
				}
			}
		}



		return colorimage;
	}

	int is_white( Mat color_image ,int x,int y){
		
		if((color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] > WHITE_MIN) && 
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] > WHITE_MIN) &&
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] > WHITE_MIN)) 
			return 1;

		else return 0;
	}

	int is_white2(Mat color_image, int x, int y){
		int c_flag = 0;
		if(is_white( color_image, x, y)){
			for(int i = 0; i < 3; i++){
				if((abs(color_image.data[ y * color_image.step + x * color_image.elemSize() + i%3]  - color_image.data[ y * color_image.step + x * color_image.elemSize() + (i+1)%3] ) < WHITE_DIF))
					c_flag++;
			}
		}
		return (c_flag == 3)? 1 : 0;
	}


	Mat thresh(Mat color_image, const openni::VideoFrameRef& depthframe){

		unsigned short* depth = (unsigned short*)depthframe.getData();
		
		for(int x = 0; x < FRAME_WIDTH; x++){
			for(int y = 0; y < FRAME_HEIGHT; y++){
				int de = depth[(y * FRAME_WIDTH) + x];
				if((de < MAX_DEPTH && de > MIN_DEPTH) && is_white2(color_image, x, y)){
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0xff;

			}

				else{

					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0;
		
				}
			}
		}



		return color_image;
	}

	int maxlist(vector<vector<Point> > cnt){
		size_t	max_num=0;
		int	max_i=0;
		for(int i = 0; i < (int)cnt.size(); i++){
			size_t cnt_num = cnt[i].size();
			if(cnt_num > max_num){
				max_num=cnt_num;
				max_i=i;
			}
		}
 
		return max_i;
		}
			
	Mat get_shuttle(Mat bin_img ,Mat color_image, const openni::VideoFrameRef& depthframe){
		
		int cx, cy;
		unsigned short cz;
		size_t max = 0;
		int max_i = 0;

		vector<vector<Point> > contours;

		findContours(bin_img ,contours, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE);

		for(int i = 0; i < (int)contours.size(); i++){
			size_t count = contours[i].size();
			if(count < MIN_SIZE || count > MAX_SIZE) continue;
			if(max < count) {
				max = count;
				max_i = i;
			}
			Rect rc = boundingRect(contours[max_i]);

			if( abs((rc.br().x - rc.tl().x) - (rc.br().y - rc.tl().y)) < ABS_MAX){


				cx = (rc.tl().x + rc.br().x) /2;
				cy = (rc.tl().y + rc.br().y) /2;
				cz = PointDistance(cx, cy , depthframe);


//				if(abs(ShuttlePoint[0] - cx) < 90 && (abs(ShuttlePoint[1] - cy) < 60)){

						ShuttlePoint[0] = cx;
						ShuttlePoint[1] = cy;
						ShuttlePoint[2] = cz;

						rectangle(color_image, rc.tl(), rc.br(), Scalar(0,0,200), 2, 4);
						
		

						break;
//					}
			}
		}
		
		return bin_img;
		
	}


	//カラーストリームを表示できる形にする
	Mat showColorStream(const openni::VideoFrameRef& colorframe)
		{
			//OpenCVの形に変換する
			Mat colorImage = Mat(colorframe.getHeight(),
								 colorframe.getWidth(),
								 CV_8UC3, (unsigned char*)colorframe.getData());

			//BGRの並びをRGBに変換する
			cvtColor(colorImage, colorImage, CV_RGB2BGR);

			return colorImage;
		}

	Mat showDepthStream(const openni::VideoFrameRef& depthframe){
		//距離データを画像化する(16bit)
		Mat depthImage = Mat(depthframe.getHeight(),
							 depthframe.getWidth(),
							 CV_16UC1, (unsigned short*)depthframe.getData());

		//0 〜 10000ミリメートルまでのデータを0〜255(8bit)にする
		depthImage.convertTo(depthImage, CV_8U, 255.0 / 10000);

		//中心点の距離を表示する
		showCenterDistance(depthImage, depthframe);

		return depthImage;
	}

	void showCenterDistance(Mat& depthImage, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();


		int centerX = videoMode.getResolutionX() / 2;
		int centerY = videoMode.getResolutionY() / 2;
		int centerIndex = (centerY * videoMode.getResolutionX()) + centerX;

		unsigned short* depth = (unsigned short*)depthframe.getData();


		stringstream ss;
		stringstream ss1;

		ss << "Center Point: " << depth[centerIndex];
	}
	
	unsigned short PointDistance(int x, int y, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();

		int pointIndex = (y * videoMode.getResolutionX()) + x;

		unsigned short* depth = (unsigned short*)depthframe.getData();

		return depth[pointIndex];


	}

	int ShuttlePoint[3];
	int Count;
	int isExist;



private:

	openni::Device device;
	openni::VideoStream colorStream;
	openni::VideoStream depthStream;

	
	
	Mat colorImage;
	Mat depthImage;
	Mat testImage;
	Mat monoImage;
	Mat textImage;
	Mat locateImage;



};

	
/* 座標の表示（演算子<<のオーバーロード）*/
/*
ostream & operator << (ostream &stream, DepthSensor point)
{
        stream << "   (" << point.ShuttlePoint[0] << ", ";
        stream << point.ShuttlePoint[1] << ", ";
        stream << point.ShuttlePoint[2] << ")\n";
        return stream;
}
*/
	



/*
int main()
{
        int n1, n2, n3;
    // ファイルのオープン
        ifstream in("test1.txt");
        if (!in) {
                cout << "***error  入力ファイルを開けません\n";
                exit(1);
        }
    
        ofstream out("test.txt");
        if (!out) {
                cout << "***error  出力ファイルを開けません\n";
                exit(1);
        }
    // データ入力と出力
        while (!in.eof()) {
                in >> n1 >> n2 >> n3;
                MyFile a(n1, n2, n3);
                out << a;
        }
    // ファイルのクローズ
        in.close();
        out.close();
    
        return 0;
}
*/


string IntToString(int number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}

string DoubleToString(double number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}



int main(int arg, const char * argv[])
{

	FILE *fp;

	if ((fp = fopen("/dev/ttyUSB0", "r+")) == NULL)
		printf("NOT CONNECT SERIAL PORT!");
	else printf("OK");

	int push_space = 0;
	string str;
	int push_key = 0;
	int go_x  = 100, go_y = 100;
	int push_x = 100, push_y = 100;
	int senterx = FRAME_WIDTH/2, sentery = FRAME_HEIGHT/2;

	TickMeter meter;

	

	if(remove("log.txt") != 0){
		printf("log.txtがありません");
		return 0;
	}

	ofstream out("log.txt");
        if (!out) {
                cout << "***error  出力ファイルを開けません\n";
                exit(1);
        }


	try{
		//OpenNIを初期化する
		openni::OpenNI::initialize();

		//センサーを初期化する
		DepthSensor sensor;
		sensor.initialize();


		//メインループ
		while(1){
			sensor.update();

			
			if(sensor.Count == COUNT_MAX && push_space){
				fprintf(fp,"A100100\n");
				printf("A100100\n");
			}

			if(sensor.isExist == 1) {

				if(push_space){

					meter.stop();



					printf("(%d, %d, %d)\n", sensor.ShuttlePoint[0], sensor.ShuttlePoint[1], sensor.ShuttlePoint[2] );

					cout << meter << endl;


					out << "(" << IntToString(sensor.ShuttlePoint[0]) << ", " << IntToString(sensor.ShuttlePoint[1]) << ", " << IntToString(sensor.ShuttlePoint[2]) << ", " << DoubleToString(meter.getTimeMilli()) << ")" << endl;


					go_x = (sensor.ShuttlePoint[0] - senterx) *100/senterx +100;
					if(sensor.ShuttlePoint[2] > 1500) go_y = 200;
					else go_y = (sensor.ShuttlePoint[2] - 500) /10+100;

					meter.reset();

					go_x = 200 - go_x;
					go_y = 200 - go_y;

					fprintf(fp,"A%6d\n",go_x * 1000 + go_y);
					printf("A%6d\n",go_x *1000 + go_y);

					meter.start();
									
				}
		
				if(sensor.Count == COUNT_MAX)
					
					printf("-----\n");
			}

			if(waitKey(10) == 13)
				while(1){
					
					push_key = waitKey(100);

					if(push_key == 97){
						push_x = 0;
						
					}
					else if(push_key == 100){
						push_x = 200;
						
					}
					else{
						push_x = 100;
						
					}
						

					if(push_key == 115){
						push_y=0;
					}
					else if(push_key == 119){
						push_y =200;
						
					}
					else{
						push_y =100;
						
					}

					if(push_key == 9) break;
					fprintf(fp,"A%6d\n",push_x * 1000 + push_y);
					printf("KEY MODE\n");

				}

			if(waitKey(10) == 32)
				push_space = 1;
			
			if(waitKey(10) == 27){
				
				fprintf(fp,"A100100\n");
				
				break;
			}
			
			printf("%d\n",waitKey(10));
		}}
	catch(std::exception&){
		std::cout << openni::OpenNI::getExtendedError() << std::endl;
	}

	out.close();

	fclose(fp);

	return 0;
}
	
			   
