#include<iostream>
#include <fstream>
#include <sstream>
#include<stdexcept>
#include<OpenNI.h>
#include<opencv2/opencv.hpp>
#include <opencv2/core/core.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/contrib/contrib.hpp> //時間計測のためのライブラリ

#include <stdio.h>
#include <math.h>
#include <string>
#include <stdlib.h>


#define FRAME_WIDTH 320 //画像サイズ 横
#define FRAME_HEIGHT 240 //    ;     縦

#define MAX_DEPTH 4000 // 取得する距離の最大値
#define MIN_DEPTH 500  //       ;       最小値


#define MAX_SIZE 120   // 白領域の最大値 
#define MIN_SIZE 5     //      ;  最小値



#define WHITE_MIN 0x00 // 白色と判定する最小値
#define WHITE_DIF 50   // RGBそれぞれの差異の最小値

#define ABS_MAX 20 

#define COUNT_MAX 15



using namespace cv;
using namespace std;

class DepthSensor
{
private:

	void changeResolution(openni::VideoStream& stream)
		{
			openni::VideoMode mode = stream.getVideoMode();
			mode.setResolution(FRAME_WIDTH, FRAME_HEIGHT);
			mode.setFps(60);
			stream.setVideoMode(mode);
			stream.setMirroringEnabled(false);
		}

public:
	void initialize(){
		
		//デバイスを習得する
		openni::Status ret = device.open( openni::ANY_DEVICE );
		if ( ret != openni::STATUS_OK){
			throw std::runtime_error( "openni::Device::open() failed.");
		}
			
		//シャトルの座標 初期化
		ShuttlePoint[0] = 0; 
		ShuttlePoint[1] = 0;
		ShuttlePoint[2] = 0;

		
		Count = 0;
		isExist = 0;

		//カラーストリームを有効にする
		colorStream.create(device, openni::SENSOR_COLOR);
		//changeResolution(colorStream);
		//colorStream.setMirroringEnabled(true);
		colorStream.start();

		//Depthストリームを有効にする
		depthStream.create(device, openni::SENSOR_DEPTH);
			
		depthStream.start();
				
	}

	//フレームの更新処理
	void update()
		{
			isExist = 1;

			static int backup_point;
			backup_point = ShuttlePoint[0];
			

			openni::VideoFrameRef colorFrame;
			openni::VideoFrameRef depthFrame;
			

			//更新されたフレームを習得する
			colorStream.readFrame( &colorFrame );
			depthStream.readFrame( &depthFrame );
			

			//フレームのデータを表示できる形に変換する
			colorImage = showColorStream( colorFrame );
//			depthImage = showDepthStream( depthFrame );


			monoImage = colorImage.clone();
			monoImage = thresh(monoImage, depthFrame);
			textImage = Mat::zeros(240, 320, CV_8UC3);

			stringstream ss;

			
			ss << "( " << ShuttlePoint[0] <<", " << ShuttlePoint[1] << ", " << ShuttlePoint[2] << ")"; 
			putText(textImage, ss.str(), Point(30, 30),
					FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255));

			locateImage = Mat::zeros(240, 320, CV_8UC3);
			
			drawLine(locateImage);

			cvNamedWindow("Color Stream", CV_WINDOW_NORMAL);
//			cvResizeWindow("Color Stream", FRAME_WIDTH*2 , FRAME_HEIGHT*2);
			cvMoveWindow("Color Stream",0, 0);

			cvNamedWindow("Locate Stream" ,CV_WINDOW_NORMAL);
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);
			cvMoveWindow("Locate Stream",500, 0);

			
//			cvResizeWindow("Depth Stream", FRAME_WIDTH*2, FRAME_HEIGHT*2);


//			openni::VideoFrameRef testFrame;
//			testStream.readFrame( &testFrame );

			cvNamedWindow("Mono Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Mono Stream",0, 400);

			cvNamedWindow("Text Stream" ,CV_WINDOW_NORMAL);			
			cvMoveWindow("Text Stream",500, 400);


			cvtColor(monoImage, monoImage,  CV_BGR2GRAY); //グレースケール変換(ここでは白黒画像のチャンネルをかえているだけ）
//			threshold(testImage, testImage, 0, 255, THRESH_BINARY|THRESH_OTSU);
			
			testImage = monoImage.clone();

			testImage = get_shuttle(testImage, colorImage, depthFrame);

			imshow("Color Stream", colorImage);
			imshow("Locate Stream", locateImage);
			imshow("Mono Stream", monoImage);
			imshow("Text Stream", textImage);

//			showCenterDistance(depthImage, depthFrame);

			if(Count > COUNT_MAX) Count = 0;

			//シャトルの位置が移動しているかどうか
			if(backup_point == ShuttlePoint[0])
				{
					isExist = 0;
					Count++;
				}
			else{
				Count=0;
			}
		}


	
	void drawLine(Mat image){
		line(image, Point(160, 0), cv::Point(160, 240), Scalar(255,255,255), 1, 4);  
		circle(image,Point(160, 300), 100, Scalar(255,255,255), 1, 4);
		circle(image,Point(160, 300), 200, Scalar(255,255,255), 1, 4);
		circle(image,Point(320 - ShuttlePoint[0],240 - 240*(ShuttlePoint[2] - MIN_DEPTH)/(MAX_DEPTH - MIN_DEPTH)), 10, Scalar(255,105,105), -1, 4);
	
	}

	//画像上のピクセルがWHITE_MINより明るいかどうか
	int is_white( Mat color_image ,int x,int y){
		
		if((color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] > WHITE_MIN) && 
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] > WHITE_MIN) &&
		   (color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] > WHITE_MIN)) 
			return 1;

		else return 0;
	}

	// RGBのどれかが強くなっていないかどうか（白に近いかどうか）
	int is_white2(Mat color_image, int x, int y){
		int c_flag = 0;
		if(is_white( color_image, x, y)){
			for(int i = 0; i < 3; i++){
				
				if(abs(
					   color_image.data[ y * color_image.step + x * color_image.elemSize() + i%3]  
					   - color_image.data[ y * color_image.step + x * color_image.elemSize() + (i+1)%3] 
					   ) < WHITE_DIF)
					
					c_flag++;
			
			}
		}
		
		return (c_flag == 3)? 1 : 0;
	}

	//MAX_DEPTHとMIN_DEPTH間の距離の領域を白、それ以外を黒にする
	Mat thresh(Mat color_image, const openni::VideoFrameRef& depthframe){

		unsigned short* depth = (unsigned short*)depthframe.getData();
		
		for(int x = 0; x < FRAME_WIDTH; x++){
			for(int y = 0; y < FRAME_HEIGHT; y++){
				int de = depth[(y * FRAME_WIDTH) + x];
				if((de < MAX_DEPTH && de > MIN_DEPTH) && is_white2(color_image, x, y)){
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0xff;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0xff;

			}

				else{

					color_image.data[ y * color_image.step + x * color_image.elemSize() + 0] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 1] = 0;
					color_image.data[ y * color_image.step + x * color_image.elemSize() + 2] = 0;
		
				}
			}
		}



		return color_image;
	}
	
	//二値化画像から白領域（シャトル）の座標を取得
	Mat get_shuttle(Mat bin_img ,Mat color_image, const openni::VideoFrameRef& depthframe){
		
		int cx, cy;
		unsigned short cz;
		size_t max = 0;
		int max_i = 0;

		vector<vector<Point> > contours;

		findContours(bin_img ,contours, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE); //輪郭抽出

		for(int i = 0; i < (int)contours.size(); i++){
			size_t count = contours[i].size();
			if(count < MIN_SIZE || count > MAX_SIZE) continue; //白領域が小さすぎる/大きすぎるものは除外
			if(max < count) {
				max = count;
				max_i = i;
			}

			Rect rc = boundingRect(contours[max_i]); //白領域を囲む短形を生成

			//短形が正方形から離れすぎていないかどうか（白領域の形が上下に長すぎないか / シャトルの形かどうか）
			if( abs((rc.br().x - rc.tl().x) - (rc.br().y - rc.tl().y)) < ABS_MAX){


				cx = (rc.tl().x + rc.br().x) /2; //短形の…
				cy = (rc.tl().y + rc.br().y) /2; //…中心点
				cz = PointDistance(cx, cy , depthframe); //短形の中心点の深度座標



				ShuttlePoint[0] = cx;
				ShuttlePoint[1] = cy;
				ShuttlePoint[2] = cz;

				rectangle(color_image, rc.tl(), rc.br(), Scalar(0,0,200), 2, 4); //短形を画像上に表示
						

						break;
//					}
			}
		}
		
		return bin_img;
		
	}


	//カラーストリームを表示できる形にする
	Mat showColorStream(const openni::VideoFrameRef& colorframe)
		{
			//OpenCVの形に変換する
			Mat colorImage = Mat(colorframe.getHeight(),
								 colorframe.getWidth(),
								 CV_8UC3, (unsigned char*)colorframe.getData());

			//BGRの並びをRGBに変換する
			cvtColor(colorImage, colorImage, CV_RGB2BGR);

			return colorImage;
		}

	Mat showDepthStream(const openni::VideoFrameRef& depthframe){
		//距離データを画像化する(16bit)
		Mat depthImage = Mat(depthframe.getHeight(),
							 depthframe.getWidth(),
							 CV_16UC1, (unsigned short*)depthframe.getData());

		//0 〜 10000ミリメートルまでのデータを0〜255(8bit)にする
		depthImage.convertTo(depthImage, CV_8U, 255.0 / 10000);

		//中心点の距離を表示する
		showCenterDistance(depthImage, depthframe);

		return depthImage;
	}

	void showCenterDistance(Mat& depthImage, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();


		int centerX = videoMode.getResolutionX() / 2;
		int centerY = videoMode.getResolutionY() / 2;
		int centerIndex = (centerY * videoMode.getResolutionX()) + centerX;

		unsigned short* depth = (unsigned short*)depthframe.getData();


		stringstream ss;
		stringstream ss1;

		ss << "Center Point: " << depth[centerIndex];
	}
	
	//(x,y)の画素の深度を取得
	unsigned short PointDistance(int x, int y, const openni::VideoFrameRef& depthframe){

		int count = 0;
		int px = 0;

		openni::VideoMode videoMode = depthStream.getVideoMode();

		int pointIndex = (y * videoMode.getResolutionX()) + x;

		unsigned short* depth = (unsigned short*)depthframe.getData();

		return depth[pointIndex];


	}

	int ShuttlePoint[3]; //シャトルの座標 ShuttlePoint([0],[1],[2]) = (x,y,z)
	int Count; // 画像上にシャトルが存在しないフレームをカウントする変数
	int isExist; // 画像上にシャトルが存在するフラグ変数


private:

	openni::Device device;
	openni::VideoStream colorStream;
	openni::VideoStream depthStream;

	
	
	Mat colorImage;
	Mat depthImage;
	Mat testImage;
	Mat monoImage;
	Mat textImage;
	Mat locateImage;



};


string IntToString(int number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}

string DoubleToString(double number)
{
  stringstream ss;
  ss << number;
  return ss.str();
}



int main(int arg, const char * argv[])
{

	FILE *fp;

	if ((fp = fopen("/dev/ttyUSB0", "r+")) == NULL)
		printf("NOT CONNECT SERIAL PORT!");
	else printf("OK");

	int push_space = 0; //Spaceキー フラグ
	string str;
	int push_key = 0;
    //シリアル通信用データ変数
	int go_x  = 100, go_y = 100; 
	int push_x = 100, push_y = 100;
	int centerx = FRAME_WIDTH/2, centery = FRAME_HEIGHT/2;

	TickMeter meter;


	if(remove("log.txt") != 0){
		printf("log.txtがありません");
		return 0;
	}

	ofstream out("log.txt");
        if (!out) {
                cout << "***error  出力ファイルを開けません\n";
                exit(1);
        }


	try{
		//OpenNIを初期化する
		openni::OpenNI::initialize();

		//センサーを初期化する
		DepthSensor sensor;
		sensor.initialize();


		//メインループ
		while(1){
			sensor.update();
			
			if(sensor.Count == COUNT_MAX && push_space){
				fprintf(fp,"A100100\n");
				printf("A100100\n");
			}

			if(sensor.isExist == 1) {

				if(push_space){

					meter.stop();



					printf("(%d, %d, %d)\n", sensor.ShuttlePoint[0], sensor.ShuttlePoint[1], sensor.ShuttlePoint[2] );

					cout << meter << endl;


					out << "(" << IntToString(sensor.ShuttlePoint[0]) 
						<< ", " << IntToString(sensor.ShuttlePoint[1]) 
						<< ", " << IntToString(sensor.ShuttlePoint[2]) 
						<< ", " << DoubleToString(meter.getTimeMilli()) 
						<< ")" << endl;

					// シリアル通信用データ生成
					go_x = (sensor.ShuttlePoint[0] - centerx) *100/centerx +100;
					if(sensor.ShuttlePoint[2] > 1500) go_y = 200;
					else go_y = (sensor.ShuttlePoint[2] - 500) /10+100;

					meter.reset();

					go_x = 200 - go_x;


					fprintf(fp,"A%6d\n",go_x * 1000 + go_y);
					printf("A%6d\n",go_x *1000 + go_y);

					meter.start();
									
				}
		
				if(sensor.Count == COUNT_MAX)
					printf("-----\n");
			}

			//Enterでキーボード操縦モード
			if(waitKey(10) == 13)
				while(1){
					
					push_key = waitKey(100);

					if(push_key == 97)
						push_x = 0;
						
					
					else if(push_key == 100)
						push_x = 200;
						
					else
						push_x = 100;
															
					if(push_key == 115)
						push_y=0;
					
					else if(push_key == 119)
						push_y =200;
						
					
					else
						push_y =100;
						

					if(push_key == 9) break;
					fprintf(fp,"A%6d\n",push_x * 1000 + push_y);
					printf("KEY MODE\n");

				}

			if(waitKey(10) == 32)
				push_space = 1;
			
			if(waitKey(10) == 27){
				
				fprintf(fp,"A100100\n");
				
				break;
			}
			
			printf("%d\n",waitKey(10));
		}}
	catch(std::exception&){
		std::cout << openni::OpenNI::getExtendedError() << std::endl;
	}

	out.close();

	fclose(fp);

	return 0;
}
	
			   
